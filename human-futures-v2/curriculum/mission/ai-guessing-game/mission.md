# ğŸ¯ Mission: Whoâ€™s the Algorithm Really Guessing?

## ğŸ’­ Prethinking Ideas
- Whatâ€™s something you've been wrongly labeled as? By who?
- How do systems group or sort people? Are those groupings fair?

## â“ Prethinking Questions
- Can AI make better decisions than people?
- What makes a classification accurate or biased?
- How do decision trees work in tech... and in society?

---

## ğŸ”§ Task 1: Sort It Out

You're the human algorithm. Youâ€™ve been given a messy dataset of [sports, artists, languages, or foods].

Your job: Sort them into categories â€” any way you want. But be ready to explain your logic.  
- What criteria did you use?
- What got left out?
- Would everyone agree with your groupings?

> ğŸ› ï¸ Materials: Sticky notes or a sorting app like Lucidchart

---

## ğŸ§  Task 2: Build a Classifier

Now you're building a **decision tree** to simulate an AI classifier.

Your job:
- Pick a category (e.g., artists, instruments, countries, sneakers, political quotes)
- Build a yes/no tree that classifies at least 8 examples
- Test it with a partner

> ğŸ§  Use tools like: [Coggle.it, Google Drawings, or paper branches]

---

## ğŸ¤– Task 3: Simulate the Algorithm

Run a **Guessing Game** powered by your tree.

- One student secretly picks an item
- Another student (acting as the AI) asks only yes/no questions from your tree to guess it
- Switch roles and refine your tree for better accuracy

---

## ğŸ’¬ Reflection

- What patterns or flaws did your decision tree reveal?
- Were there moments where human reasoning outperformed your AI structure?
- How does this exercise relate to the way social media, policing, or job filters classify people?
- What happens when the training data leaves someone out?

---

## ğŸ¨ Remix

- Redesign your classifier for justice: How would you rework the logic to be more fair, inclusive, or community-centered?
- Build a tree for activism: How would you classify protest strategies, or climate actions, or media messages?
- Create a â€œBias Detectorâ€ tree â€” where each leaf tells you the kind of bias a system might be making.

---
